{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "78835a1a-cd32-46b3-a8a1-4870ceba9c4c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import nad Functions"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, StructType, StructField, NumericType, IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "from math import radians, sqrt, hypot, pi, sin, cos, asin, atan2\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import tempfile, shutil, requests, warnings, logging, glob, json\n",
    "from functools import reduce\n",
    "from itertools import product\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "end = '2024-12-01'\n",
    "end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "start_date = end_date - relativedelta(years=2)\n",
    "start_date = datetime(start_date.year, start_date.month, 1)\n",
    "start = start_date.strftime('%Y-%m-%d')\n",
    "\n",
    "state_names = [\n",
    "    'NJ_NY_PA', 'CT_IL_TX', 'IN_MI_TN', 'AK_AL_AR_AZ_CA', 'CO_DE_DL_FL_GA_HI', 'IA_ID_KS_KY_LA_MA_MD', \n",
    "    'ME_MN_MO_MS_MT_NE_NV_NH_NM_NC_ND', 'OH_OK_OR_RI_SC', 'SD_UT_VT_VA_WA_WV_WI_WY'\n",
    "]\n",
    "\n",
    "states = [\n",
    "    ['NJ', 'NY', 'PA'], ['CT', 'IL', 'TX'], ['IN', 'MI', 'TN'], ['AK', 'AL', 'AR', 'AZ', 'CA'], \n",
    "    ['CO', 'DE', 'DL', 'FL', 'GA', 'HI'], ['IA', 'ID', 'KS', 'KY', 'LA', 'MA', 'MD'], \n",
    "    ['ME', 'MN', 'MO', 'MS', 'MT', 'NE', 'NV', 'NH', 'NM', 'NC', 'ND'], \n",
    "    ['OH', 'OK', 'OR', 'RI', 'SC'], ['SD', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "]\n",
    "\n",
    "################################################################################################\n",
    "# azuremaps api for coordinates generation\n",
    "################################################################################################\n",
    "def azuremaps_api(location):\n",
    "    endpoint = 'https://atlas.microsoft.com/search/address/json'\n",
    "    api_key = '4t6tMJFedI3Dag3Ewu1oBYmpa3jqiKhlG12FUxe9Vns'\n",
    "    params = {\"subscription-key\": api_key, \"api-version\": \"1.0\", \"query\": location, \"countrySet\": \"US\", \"top\": 1, \"typeahead\": True, \"extendedPostalCodesFor\": \"Addr\", \"includeEntityTypes\": \"Address\", \"includeIso2\": True}\n",
    "\n",
    "    response = requests.get(endpoint, params)\n",
    "    if response.status_code == 200 and len(response.json()[\"results\"])>0:\n",
    "        data = response.json()\n",
    "        lat = data[\"results\"][0][\"position\"][\"lat\"]\n",
    "        lon = data[\"results\"][0][\"position\"][\"lon\"]\n",
    "        return dict(latitude=lat, longitude=lon)\n",
    "    else:\n",
    "        return dict(latitude=None, longitude=None)\n",
    "\n",
    "################################################################################################\n",
    "# haversine distance calculation\n",
    "################################################################################################\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    d_lat = lat2_rad - lat1_rad\n",
    "    d_lon = lon2_rad - lon1_rad\n",
    "    a = sin(d_lat / 2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(d_lon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = 3958.8 * c  # Earth's radius in miles (approximate)\n",
    "    return distance\n",
    "\n",
    "################################################################################################\n",
    "# Tomforth api\n",
    "################################################################################################\n",
    "def tomforth_api(lat, lon, radius):\n",
    "    api_url = 'https://ringpopulationsapi.azurewebsites.net/api/globalringpopulations'\n",
    "    params = {\n",
    "        'latitude': lat,\n",
    "        'longitude': lon,\n",
    "        'distance_km': radius # integer only\n",
    "    }\n",
    "    response = requests.get(api_url, params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "################################################################################################\n",
    "# Generate Coordinate for dealer registry\n",
    "################################################################################################\n",
    "def generate_dealer_coordinates(dealers, coordinates):\n",
    "    dealers = pd.merge(dealers, coordinates, on=['DEALER_ADDRESS_FULL'], how='left')\n",
    "\n",
    "    out=[]\n",
    "    for i, row in dealers.iterrows():\n",
    "        # print('6 -----   ',row['LATITUDE'])\n",
    "        if str(row['LATITUDE'])=='nan':\n",
    "            location_info = azuremaps_api(row['DEALER_ADDRESS_FULL'])\n",
    "            # print('2 ------  ', location_info)\n",
    "            if location_info['latitude'] is not None and location_info['longitude'] is not None:\n",
    "                out.append({'DEALER_ADDRESS_FULL': row['DEALER_ADDRESS_FULL'], 'LATITUDE': location_info['latitude'], 'LONGITUDE': location_info['longitude']})\n",
    "            else:\n",
    "                out.append({'DEALER_ADDRESS_FULL': row['DEALER_ADDRESS_FULL'], 'LATITUDE': None, 'LONGITUDE': None})\n",
    "        else:\n",
    "            out.append({'DEALER_ADDRESS_FULL': row['DEALER_ADDRESS_FULL'], 'LATITUDE': row['LATITUDE'], 'LONGITUDE': row['LONGITUDE']})\n",
    "    out = pd.DataFrame(out)\n",
    "    print(f'{len(out)} - {len(coordinates)} = {len(out) - len(coordinates)} rows added. {len(out[out.isna().any(axis=1)])} null rows out of {len(out)}')\n",
    "\n",
    "    return out\n",
    "\n",
    "################################################################################################\n",
    "# Generate Dealer Population\n",
    "################################################################################################\n",
    "def generate_dealer_population(dealers, coordinates, pops_df):\n",
    "    # states = ['OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY']\n",
    "    df = pd.merge(coordinates.dropna(), pops_df.drop_duplicates(subset='DEALER_ADDRESS_FULL'), on=['DEALER_ADDRESS_FULL'], how='left')\n",
    "    df = pd.merge(df, dealers[['DEALER_ADDRESS_FULL', 'DEALER_STATE_ABBRV']], on=['DEALER_ADDRESS_FULL'], how='left')\n",
    "\n",
    "    print(f'{len(df[df.isna().any(axis=1)])} null rows out of {len(df)} in population data')\n",
    "    # df = df[df['DEALER_STATE_ABBRV'].isin(states)]\n",
    "    print(f'{len(df[df.isna().any(axis=1)])} null rows out of {len(df)} in selected population data')\n",
    "\n",
    "    new_pops_df = pd.DataFrame()\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc='Processing'):\n",
    "        if pd.isna(row['people']):\n",
    "            try:\n",
    "                pop_df = tomforth_api(lat=row['LATITUDE'], lon=row['LONGITUDE'], radius=161).drop(columns='name')\n",
    "                pop_df['DEALER_ADDRESS_FULL'] = row['DEALER_ADDRESS_FULL']\n",
    "                new_pops_df = pd.concat([new_pops_df, pop_df])\n",
    "            except:\n",
    "                continue\n",
    "    upd_pops_df = pd.concat([pops_df, new_pops_df])\n",
    "    print(f'{len(upd_pops_df)} - {len(pops_df)} = {len(upd_pops_df) - len(pops_df)} rows added. {len(upd_pops_df[upd_pops_df.isna().any(axis=1)])} null rows out of {len(upd_pops_df)}')\n",
    "    return upd_pops_df\n",
    "\n",
    "################################################################################################\n",
    "# optimal radius calculation for a dealers\n",
    "################################################################################################\n",
    "\n",
    "def count_addresses_in_state_make(polk_df_n, selected_dealers):\n",
    "    selected_dealers_spark = spark.createDataFrame(selected_dealers)\n",
    "    # Join selected_dealers with polk_df_n on MAKE and DEALER_STATE_ABBRV\n",
    "    joined_df = selected_dealers_spark.join(\n",
    "        polk_df_n,\n",
    "        (selected_dealers_spark['MAKE'] == polk_df_n['MAKE']) & \n",
    "        (selected_dealers_spark['DEALER_STATE_ABBRV'] == polk_df_n['DEALER_STATE_ABBRV']),\n",
    "        how='inner'\n",
    "    )\n",
    "    # Group by the columns in selected_dealers and count the distinct DEALER_ADDRESS_FULL from polk_df_n\n",
    "    count_df = joined_df.groupBy(\n",
    "        selected_dealers_spark['DEALER_ADDRESS_FULL'],\n",
    "        selected_dealers_spark['MAKE'],\n",
    "        selected_dealers_spark['DEALER_STATE_ABBRV']\n",
    "    ).agg(F.countDistinct(polk_df_n['DEALER_ADDRESS_FULL']).alias('distinct_count_in_polk_df'))\n",
    "\n",
    "    # Convert to pandas and merge with the original selected_dealers DataFrame\n",
    "    count_df_pd = count_df.toPandas()\n",
    "\n",
    "    return count_df_pd\n",
    "\n",
    "def dealer_to_dealer_distance(all_dealers, coordinates):\n",
    "    # Create a DataFrame for dealers and join with coordinates\n",
    "    base_df = spark.createDataFrame(all_dealers).select('DEALER_NAME', 'DEALER_ADDRESS_FULL', 'MAKE')\n",
    "    base_df = base_df.join(coordinates.dropna(), on=['DEALER_ADDRESS_FULL'])\n",
    "    \n",
    "    # Create aliases for self join\n",
    "    df1 = base_df.alias('df1')\n",
    "    df2 = base_df.alias('df2')\n",
    "    \n",
    "    print('Applying Cross Join')\n",
    "    # Cross join the dataframe with itself\n",
    "    cross_df = df1.crossJoin(df2)\n",
    "    \n",
    "    # Filter out rows where addresses are the same or dealer names are the same,\n",
    "    # and ensure that the MAKE is the same for both rows.\n",
    "    filtered_df = cross_df.where(\n",
    "        (F.col(\"df1.DEALER_NAME\") != F.col(\"df2.DEALER_NAME\")) &\n",
    "        (F.col(\"df1.DEALER_ADDRESS_FULL\") != F.col(\"df2.DEALER_ADDRESS_FULL\")) &\n",
    "        (F.col(\"df1.MAKE\") == F.col(\"df2.MAKE\"))\n",
    "    )\n",
    "    \n",
    "    print('Calculating Distance')\n",
    "    # Convert degrees to radians for both sets of coordinates\n",
    "    distance_df = filtered_df.withColumn(\"lat1\", F.radians(F.col(\"df1.LATITUDE\"))) \\\n",
    "        .withColumn(\"lon1\", F.radians(F.col(\"df1.LONGITUDE\"))) \\\n",
    "        .withColumn(\"lat2\", F.radians(F.col(\"df2.LATITUDE\"))) \\\n",
    "        .withColumn(\"lon2\", F.radians(F.col(\"df2.LONGITUDE\")))\n",
    "    \n",
    "    # Earth's radius in kilometers (or use miles if preferred)\n",
    "    # r = 6371.0\n",
    "\n",
    "    # Earth's radius in miles (or use miles if preferred)\n",
    "    r = 3963.1\n",
    "\n",
    "    # Calculate haversine distance using Spark SQL functions\n",
    "    distance_df = distance_df.withColumn(\n",
    "        \"DISTANCE\",\n",
    "        2 * F.lit(r) * F.asin(\n",
    "            F.sqrt(\n",
    "                F.pow(F.sin((F.col(\"lat2\") - F.col(\"lat1\")) / 2), 2) +\n",
    "                F.cos(F.col(\"lat1\")) * F.cos(F.col(\"lat2\")) *\n",
    "                F.pow(F.sin((F.col(\"lon2\") - F.col(\"lon1\")) / 2), 2)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Select and rename the desired columns, ensuring DISTANCE is cast to double.\n",
    "    result_df = distance_df.select(\n",
    "        F.col(\"df1.DEALER_NAME\").alias(\"DEALER_NAME\"),\n",
    "        F.col(\"df1.DEALER_ADDRESS_FULL\").alias(\"DEALER_ADDRESS_FULL\"),\n",
    "        F.col(\"df1.MAKE\").alias(\"MAKE\"),\n",
    "        F.col(\"df2.DEALER_NAME\").alias(\"COMPARED_DEALER_NAME\"),\n",
    "        F.col(\"df2.DEALER_ADDRESS_FULL\").alias(\"COMPARED_DEALER_ADDRESS_FULL\"),\n",
    "        F.col(\"df2.MAKE\").alias(\"COMPARED_MAKE\"),\n",
    "        F.col(\"DISTANCE\").cast(\"double\")\n",
    "    )\n",
    "    \n",
    "    # Optionally, join with the original dealer DataFrame to filter or enrich the data.\n",
    "    final_df = result_df.join(\n",
    "        base_df.select('DEALER_NAME', 'DEALER_ADDRESS_FULL', 'MAKE'),\n",
    "        on=['DEALER_NAME', 'DEALER_ADDRESS_FULL', 'MAKE']\n",
    "    )\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def dealer_optimal_radius(polk_df, coordinates, state_pop, highlighted_dealer, pops_df):\n",
    "    # Storing the latitude and longitude values of the selected dealer\n",
    "    dealer_radius_factor = dict()\n",
    "    for key, value in coordinates.filter(coordinates['DEALER_ADDRESS_FULL'] == highlighted_dealer['DEALER_ADDRESS_FULL']).drop('DEALER_ADDRESS_FULL').first().asDict().items():\n",
    "        dealer_radius_factor[key] = value\n",
    "\n",
    "    # Getting the list of full addresses of all the dealers who sells the brand same as our target dealer\n",
    "    # brand_dealers = list(polk_df.filter((F.col('REG_TYPE')=='N') & (F.col('MAKE')==highlighted_dealer['MAKE'])).select(\"DEALER_ADDRESS_FULL\").distinct().rdd.map(lambda row: row[0]).collect())\n",
    "\n",
    "    # Count of dealers in the same state as our target dealer\n",
    "    # state_brand_dealer_count = polk_df.filter((F.col('REG_TYPE')=='N') & (F.col('MAKE')==highlighted_dealer['MAKE']) & (F.col('DEALER_STATE_ABBRV')==highlighted_dealer['DEALER_STATE_ABBRV'])).select('DEALER_ADDRESS_FULL').distinct().count()\n",
    "    state_brand_dealer_count = count_df_pd[count_df_pd['DEALER_ADDRESS_FULL'] == highlighted_dealer['DEALER_ADDRESS_FULL']]['distinct_count_in_polk_df'].iloc[0]\n",
    "\n",
    "    # print('------------>>>>>>>>',highlighted_dealer['DEALER_ADDRESS_FULL'], state_brand_dealer_count)\n",
    "\n",
    "    # Getting the state population for the state of target dealer and appending into the 'dealer_radius_factor' dictionary\n",
    "    dealer_radius_factor['state_pop'] = state_pop[state_pop['state_code']==highlighted_dealer['DEALER_STATE_ABBRV']]['pop2022'].values[0]\n",
    "    \n",
    "    # Dealer's optimal population = State population * Factor / Brand Dealers in State\n",
    "    for factor in [15]:\n",
    "        radius_factor = dealer_radius_factor['state_pop'] * factor / state_brand_dealer_count\n",
    "        \n",
    "        if highlighted_dealer['DEALER_ADDRESS_FULL'] in list(pops_df['DEALER_ADDRESS_FULL']):\n",
    "            pop_df = pops_df[pops_df['DEALER_ADDRESS_FULL']==highlighted_dealer['DEALER_ADDRESS_FULL']]\n",
    "        else:\n",
    "            pop_df = tomforth_api(lat=dealer_radius_factor['LATITUDE'], lon=dealer_radius_factor['LONGITUDE'], radius=161).drop(columns='name')\n",
    "        # converting km to miles using 0.62137119\n",
    "        pop_df['distance_mi'] = round(pop_df['distance'] * 0.62137119, 0)\n",
    "\n",
    "        # the difference between actual population and optimal population gives the best place to be in\n",
    "        # the best difference will be zero means optimal population is equal to actual population\n",
    "        pop_df['difference'] = abs(pop_df['people'] - radius_factor)\n",
    "\n",
    "        # display(pop_df)\n",
    "        # finding the row where difference column of pop_df is minimum\n",
    "        highlight = pop_df.loc[pop_df['difference'].idxmin()]\n",
    "\n",
    "        dealer_radius_factor[f'optimal_radius_{factor}'] = highlight['distance_mi']\n",
    "        dealer_radius_factor[f'population_{factor}'] = highlight['people']\n",
    "        # print(dealer_radius_factor)\n",
    "        # print(type(dealer_radius_factor[[f'optimal_radius_{factor}', f'population_{factor}']]))\n",
    "\n",
    "    return dealer_radius_factor[f'optimal_radius_{factor}'], dealer_radius_factor[f'population_{factor}']\n",
    "\n",
    "################################################################################################\n",
    "# optimal radius calculation for all new dealers\n",
    "################################################################################################\n",
    "def calculate_optimal_radius(selected_dealers):\n",
    "    print('Nunmber of New rows - ', selected_dealers.shape)\n",
    "    out=[]\n",
    "    for i, row in tqdm(selected_dealers.iterrows(), total=len(selected_dealers), desc='Processing'):\n",
    "        highlighted_dealer = row.to_dict()\n",
    "        highlighted_dealer['State'] = state_codes[state_codes['state_code']==highlighted_dealer['DEALER_STATE_ABBRV']]['State'].iloc[0]\n",
    "        # print(type(highlighted_dealer))\n",
    "        radius, population = dealer_optimal_radius(polk_df=polk_df, state_pop=pd.merge(pop2022, state_codes), highlighted_dealer=highlighted_dealer, coordinates=dealer_coordinates, pops_df=pops_df)\n",
    "\n",
    "        highlighted_dealer['RADIUS'] = radius\n",
    "        highlighted_dealer['POPULATION'] = population\n",
    "        out.append(highlighted_dealer)\n",
    "\n",
    "    optimal_radius_df = pd.DataFrame(out)\n",
    "    if not optimal_radius_df.empty:\n",
    "        optimal_radius_df = optimal_radius_df[['DEALER_ADDRESS_FULL', 'RADIUS', 'POPULATION']]\n",
    "    return optimal_radius_df\n",
    "    # optimal_radius_df_final = pd.concat([optimal_radius_df, oprad_all], axis=0)\n",
    "    # return optimal_radius_df_final\n",
    "\n",
    "def calculate_optimal_radius_new(selected_dealers, state_codes, count_df_pd, pop2022, polk_df, dealer_coordinates, pops_df):\n",
    "\n",
    "    state_pop = pd.merge(pop2022, state_codes).rename(columns={'state_code':'DEALER_STATE_ABBRV'}).drop('State', axis=1)\n",
    "    selected_dealers = selected_dealers.merge(state_codes.rename(columns={'state_code':'DEALER_STATE_ABBRV'}), on='DEALER_STATE_ABBRV')\n",
    "\n",
    "    # Storing the latitude and longitude values of the selected dealer\n",
    "    dealer_radius_factor = dealer_coordinates.toPandas().merge(selected_dealers, on=['DEALER_ADDRESS_FULL'], how='inner')\n",
    "    dealer_radius_factor = dealer_radius_factor.merge(count_df_pd.drop_duplicates(subset=['DEALER_ADDRESS_FULL']), on=['DEALER_ADDRESS_FULL', 'DEALER_STATE_ABBRV'], how='inner')\n",
    "\n",
    "    # Getting the state population for the state of target dealer and appending into the 'dealer_radius_factor' dictionary\n",
    "    dealer_radius_factor = dealer_radius_factor.merge(state_pop, on=['DEALER_STATE_ABBRV'], how='inner')\n",
    "    \n",
    "    # Dealer's optimal population = State population * Factor / Brand Dealers in State\n",
    "    factor = 15\n",
    "    dealer_radius_factor['radius_factor'] = dealer_radius_factor['pop2022'] * factor / dealer_radius_factor['distinct_count_in_polk_df']\n",
    "\n",
    "    old_add = set(pops_df['DEALER_ADDRESS_FULL'])\n",
    "    def generate_pop_df(row, old_add):\n",
    "        if row['DEALER_ADDRESS_FULL'] in old_add:\n",
    "            pop_df = pops_df[pops_df['DEALER_ADDRESS_FULL']==row['DEALER_ADDRESS_FULL']]\n",
    "        else:\n",
    "            pop_df = tomforth_api(lat=row['LATITUDE'], lon=row['LONGITUDE'], radius=161).drop(columns='name')\n",
    "\n",
    "        # converting km to miles using 0.62137119\n",
    "        pop_df['distance_mi'] = round(pop_df['distance'] * 0.62137119, 0)\n",
    "\n",
    "        # the difference between actual population and optimal population gives the best place to be in\n",
    "        # the best difference will be zero means optimal population is equal to actual population\n",
    "        pop_df['difference'] = abs(pop_df['people'] - row['radius_factor'])\n",
    "\n",
    "        # finding the row where difference column of pop_df is minimum\n",
    "        highlight = pop_df.loc[pop_df['difference'].idxmin()]\n",
    "\n",
    "        return highlight['distance_mi'], highlight['people']\n",
    "\n",
    "    dealer_radius_factor[['RADIUS', 'POPULATION']] = dealer_radius_factor.progress_apply(\n",
    "        lambda row: generate_pop_df(row, old_add), axis=1, result_type='expand')\n",
    "\n",
    "    optimal_radius_df = dealer_radius_factor[['DEALER_ADDRESS_FULL', 'RADIUS', 'POPULATION']]\n",
    "    return optimal_radius_df\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# Tuning Optimal Radius, Local Dealers and Population\n",
    "################################################################################################\n",
    "\n",
    "def merge_and_get_dealers_count(optimal_radius_df_final1, d2d):\n",
    "\n",
    "    # Merge optimal_radius_df_final1 with d2d on DEALER_NAME and DEALER_ADDRESS_FULL\n",
    "    merged_df = optimal_radius_df_final1.merge(\n",
    "        d2d, \n",
    "        on=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"], \n",
    "        how=\"left\")\n",
    "\n",
    "    # Filter rows where DISTANCE is within Local Radius\n",
    "    filtered_df = merged_df[merged_df[\"DISTANCE\"] <= merged_df[\"Local Radius\"]]\n",
    "\n",
    "    # Remove duplicate (COMPARED_DEALER_ADDRESS_FULL, COMPARED_DEALER_NAME) pairs\n",
    "    filtered_df = filtered_df.drop_duplicates(\n",
    "        subset=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"COMPARED_DEALER_ADDRESS_FULL\", \"COMPARED_DEALER_NAME\"])\n",
    "\n",
    "    # Count unique pairs per dealer\n",
    "    n_local_dealers = (\n",
    "        filtered_df.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"N Local Dealers\"))\n",
    "\n",
    "    # Merge the computed counts back to the original DataFrame\n",
    "    optimal_radius_df_final1 = optimal_radius_df_final1.merge(\n",
    "        n_local_dealers, \n",
    "        on=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"], \n",
    "        how=\"left\"\n",
    "    ).fillna({\"N Local Dealers\": 0})  # Fill NaN values with 0 if no local dealers found\n",
    "\n",
    "    return optimal_radius_df_final1\n",
    "\n",
    "def fixing_more_than_30_local_dealers(hilo, d2d):\n",
    "    hilo = optimal_radius_df_final1\n",
    "    hilo_0 = hilo[(hilo['N Local Dealers']<=0) & (hilo['Local Radius']<=100)]\n",
    "    hilo_30 = hilo[hilo['N Local Dealers']>30]\n",
    "    # print(hilo_0.shape)\n",
    "    # print(hilo_30.shape)\n",
    "\n",
    "    d2d_30 = d2d.merge(hilo_30[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner')\n",
    "    d2d_0 = d2d.merge(hilo_0[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner')\n",
    "    # print(d2d_30.drop_duplicates(['DEALER_NAME', 'DEALER_ADDRESS_FULL']).shape)\n",
    "    # print(d2d_0.drop_duplicates(['DEALER_NAME', 'DEALER_ADDRESS_FULL']).shape)\n",
    "\n",
    "\n",
    "    # Sort values by DEALER_ADDRESS_FULL and DISTANCE\n",
    "    df_sorted = d2d_30.sort_values(by=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"DISTANCE\"])\n",
    "\n",
    "    # Remove rows where DISTANCE > 100 before selecting the 30th row\n",
    "    df_filtered = df_sorted[df_sorted[\"DISTANCE\"] <= 100*1.60934]\n",
    "\n",
    "    # Get the 30th row per group (nth index 29) if at least 30 rows exist, otherwise take the last available row\n",
    "    df_30th = df_filtered.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"]).nth(29).reset_index()\n",
    "\n",
    "    # If a group has fewer than 30 rows, take the last available row\n",
    "    df_last = df_filtered.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"]).tail(1)  # Get the last row per group\n",
    "    df_30th = pd.concat([df_30th, df_last]).drop_duplicates([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"])\n",
    "    print(df_30th.columns)\n",
    "\n",
    "    # Count unique COMPARED_DEALER_ADDRESS_FULL within the selected DISTANCE ---------------------------\n",
    "\n",
    "    # Merge df_30th with df_filtered on DEALER_NAME and DEALER_ADDRESS_FULL\n",
    "    merged_df = df_30th[['DEALER_NAME', 'DEALER_ADDRESS_FULL', 'MAKE', 'DISTANCE']].merge(\n",
    "        df_filtered, \n",
    "        on=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"], \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Filter rows where DISTANCE is within the allowed range\n",
    "    filtered_df = merged_df[merged_df[\"DISTANCE_y\"] <= merged_df[\"DISTANCE_x\"]]\n",
    "\n",
    "    # Remove duplicate (COMPARED_DEALER_ADDRESS_FULL, COMPARED_DEALER_NAME) pairs per dealer\n",
    "    filtered_df = filtered_df.drop_duplicates(\n",
    "        subset=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"COMPARED_DEALER_ADDRESS_FULL\", \"COMPARED_DEALER_NAME\"]\n",
    "    )\n",
    "\n",
    "    # Count unique dealer pairs per (DEALER_NAME, DEALER_ADDRESS_FULL)\n",
    "    dealer_counts = (\n",
    "        filtered_df.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"DEALER_COUNT\")\n",
    "    )\n",
    "\n",
    "    # Merge the computed counts back to the original DataFrame\n",
    "    df_30th = df_30th.merge(\n",
    "        dealer_counts, \n",
    "        on=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"], \n",
    "        how=\"left\"\n",
    "    ).fillna({\"DEALER_COUNT\": 0})  # Fill NaN values with 0 if no matches\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    df_30th = df_30th[[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"DISTANCE\", \"DEALER_COUNT\"]]\n",
    "\n",
    "    df_30th['DISTANCE'] = np.floor(df_30th['DISTANCE']).astype(int)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    # Sort values by DEALER_ADDRESS_FULL and DISTANCE\n",
    "    df_sorted = d2d_0.sort_values(by=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"DISTANCE\"])\n",
    "\n",
    "    # Get the top 10 rows per group where DISTANCE ≤ 100 miles\n",
    "    df_top10 = df_sorted[df_sorted[\"DISTANCE\"] <= 100*1.60934].groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"]).head(10)\n",
    "\n",
    "    # Compute the maximum distance within these top 10 rows\n",
    "    df_0th = df_top10.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"])[\"DISTANCE\"].max().reset_index()\n",
    "\n",
    "    # If a dealer has fewer than 10 valid distances, set DISTANCE to 100 miles\n",
    "    df_0th[\"DISTANCE\"] = df_0th[\"DISTANCE\"].fillna(100*1.60934)\n",
    "\n",
    "    # Count unique COMPARED_DEALER_ADDRESS_FULL within the selected DISTANCE ---------------------------\n",
    "\n",
    "    # Merge df_30th with df_filtered on DEALER_NAME and DEALER_ADDRESS_FULL\n",
    "    merged_df = df_0th[['DEALER_NAME', 'DEALER_ADDRESS_FULL', 'DISTANCE']].merge(\n",
    "        df_sorted, \n",
    "        on=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"], \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Filter rows where DISTANCE is within the allowed range\n",
    "    filtered_df = merged_df[merged_df[\"DISTANCE_y\"] <= merged_df[\"DISTANCE_x\"]]\n",
    "\n",
    "    # Remove duplicate (COMPARED_DEALER_ADDRESS_FULL, COMPARED_DEALER_NAME) pairs per dealer\n",
    "    filtered_df = filtered_df.drop_duplicates(\n",
    "        subset=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"COMPARED_DEALER_ADDRESS_FULL\", \"COMPARED_DEALER_NAME\"]\n",
    "    )\n",
    "\n",
    "    # Count unique dealer pairs per (DEALER_NAME, DEALER_ADDRESS_FULL)\n",
    "    dealer_counts = (\n",
    "        filtered_df.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"DEALER_COUNT\")\n",
    "    )\n",
    "\n",
    "    # Merge the computed counts back to the original DataFrame\n",
    "    df_0th = df_0th.merge(\n",
    "        dealer_counts, \n",
    "        on=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"], \n",
    "        how=\"left\"\n",
    "    ).fillna({\"DEALER_COUNT\": 0})  # Fill NaN values with 0 if no matches\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    df_0th = df_0th[[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"DISTANCE\", \"DEALER_COUNT\"]]\n",
    "\n",
    "    df_0th['DISTANCE'] = np.ceil(df_0th['DISTANCE']).astype(int)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    # Find rows in df1 that are NOT in df2 based on columns 'DEALER_NAME', 'DEALER_ADDRESS_FULL'\n",
    "    df_0th_100_add = df_sorted.merge(df_top10[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='left', indicator=True)\n",
    "    df_0th_100_add = df_0th_100_add[df_0th_100_add['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    df_0th_100 = df_0th_100_add.drop_duplicates(subset=['DEALER_NAME', 'DEALER_ADDRESS_FULL'])\n",
    "    df_0th_100 = df_0th_100[['DEALER_NAME', 'DEALER_ADDRESS_FULL',]]\n",
    "\n",
    "    df_0th_100['DISTANCE'] = 100*1.60934\n",
    "    df_0th_100['DISTANCE'] = np.ceil(df_0th_100['DISTANCE']).astype(int)\n",
    "    df_0th_100['DEALER_COUNT'] = 0\n",
    "\n",
    "    # print(df_0th_100.columns, df_0th_100.shape)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    df_30th_0th = pd.concat([df_30th, df_0th, df_0th_100], axis=0)\n",
    "    # print(df_30th_0th.info())\n",
    "\n",
    "    df_30th_0th['DISTANCE'] = df_30th_0th['DISTANCE'].where(df_30th_0th['DISTANCE'] <= 100*1.60934, 100*1.60934)\n",
    "\n",
    "    # Distance in KM\n",
    "    # df_30th_0th['DISTANCE'] = df_30th_0th['DISTANCE']*1.60934\n",
    "    df_30th_0th['DISTANCE'] = df_30th_0th['DISTANCE'].astype(int)\n",
    "\n",
    "    df_30th_0th = df_30th_0th.merge(pops_df[['distance', 'people', 'DEALER_NAME', 'DEALER_ADDRESS_FULL']], left_on=['DEALER_NAME', 'DEALER_ADDRESS_FULL', 'DISTANCE'], right_on=['DEALER_NAME', 'DEALER_ADDRESS_FULL', 'distance'], how='left').sort_values(by='distance').drop(columns='distance')\n",
    "\n",
    "    df_30th_0th = df_30th_0th.drop_duplicates()\n",
    "\n",
    "    return df_30th_0th\n",
    "\n",
    "def update_original_df(optimal_radius_df_final1, df_30th_0th):\n",
    "\n",
    "    hilo_final = optimal_radius_df_final1.copy()\n",
    "\n",
    "    hilo_final_1 = hilo_final.merge(df_30th_0th[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='left', indicator=True)\n",
    "    hilo_final_1 = hilo_final_1[hilo_final_1['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    # hilo_final_1.DEALER_ADDRESS_FULL.nunique()\n",
    "\n",
    "\n",
    "    difference_df = df_30th_0th.merge(hilo_final[[\n",
    "    'DEALER_NAME', 'DEALER_ADDRESS', 'DEALER_ADDRESS_FULL', 'MAKE',\n",
    "    'Local Radius', 'Local Population', 'N Local Dealers']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner').drop_duplicates()\n",
    "\n",
    "    difference_df.loc[:, ['Local Radius', 'N Local Dealers', 'Local Population']] = difference_df[['DISTANCE', 'DEALER_COUNT', 'people']].values\n",
    "\n",
    "    difference_df.drop(columns=['DISTANCE', 'DEALER_COUNT', 'people'], inplace=True)\n",
    "\n",
    "    hilo_final = pd.concat([hilo_final_1, difference_df], axis=0)\n",
    "    hilo_final.drop_duplicates(['DEALER_NAME', 'DEALER_ADDRESS_FULL']).shape\n",
    "    hilo_final = hilo_final.merge(dealer_registry[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner', indicator=True)\n",
    "    hilo_final = hilo_final[hilo_final['_merge'] == 'both'].drop(columns=['_merge'])\n",
    "    hilo_final.drop_duplicates(['DEALER_NAME', 'DEALER_ADDRESS_FULL']).shape\n",
    "\n",
    "    return hilo_final\n",
    "\n",
    "def population_constraint_fix(population_constraint, hilo_final, d2d, pops_df):\n",
    "\n",
    "    hilo_final_1_10mil = hilo_final[hilo_final['Local Population']>population_constraint]\n",
    "    hilo_final_1_10mil = hilo_final_1_10mil[['DEALER_NAME', 'DEALER_ADDRESS', 'DEALER_ADDRESS_FULL', 'Local Radius', 'N Local Dealers', 'Local Population']]\n",
    "    print(hilo_final_1_10mil.info())\n",
    "\n",
    "    pops_df_10mil = pops_df.merge(hilo_final_1_10mil[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner')\n",
    "    pops_df_10mil = pops_df_10mil[(pops_df_10mil['distance']<100*1.60934) & (pops_df_10mil['people']<10000000)]\n",
    "    pops_df_10mil = (\n",
    "        pops_df_10mil.loc[pops_df_10mil.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"])[\"people\"].idxmax()]\n",
    "        [[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"distance\", \"people\"]]\n",
    "    )\n",
    "\n",
    "    d2d_10mil = d2d.merge(pops_df_10mil[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner')\n",
    "\n",
    "    # Merge the two DataFrames on DEALER_ADDRESS_FULL\n",
    "    merged_df = pd.merge(d2d_10mil, pops_df_10mil, on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], suffixes=('_d2d', '_pops'))\n",
    "    # Filter rows where the DISTANCE in d2d_10mil is less than or equal to the distance in pops_df_10mil\n",
    "    filtered_df = merged_df[merged_df['DISTANCE'] <= merged_df['distance']]\n",
    "    # Count the number of COMPARED_DEALER_ADDRESS_FULL for each DEALER_ADDRESS_FULL\n",
    "    result = (\n",
    "        filtered_df.groupby(['DEALER_NAME', 'DEALER_ADDRESS_FULL'])\n",
    "        .agg({'COMPARED_DEALER_ADDRESS_FULL': 'count'})\n",
    "        .rename(columns={'COMPARED_DEALER_ADDRESS_FULL': 'count_compared_dealers'})\n",
    "    )\n",
    "    result = result.reset_index()\n",
    "\n",
    "\n",
    "    result2 = hilo_final_1_10mil.merge(result, on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='left', indicator=True)\n",
    "    result2 = result2[result2['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    result2 = result2[['DEALER_NAME', 'DEALER_ADDRESS_FULL']].drop_duplicates()\n",
    "    result2['count_compared_dealers'] = 0\n",
    "\n",
    "    result = pd.concat([result, result2], axis=0)\n",
    "    result = result.sort_values(by='count_compared_dealers', ascending=False)\n",
    "    result = result.drop_duplicates(subset=['DEALER_NAME', 'DEALER_ADDRESS_FULL'])\n",
    "\n",
    "\n",
    "    pops_df_10mil = pops_df_10mil.merge(result, on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'])\n",
    "    pops_df_10mil.rename(columns={'distance':'DISTANCE', 'count_compared_dealers':'DEALER_COUNT'}, inplace=True)\n",
    "\n",
    "    hilo_final_1 = hilo_final.merge(pops_df_10mil[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='left', indicator=True)\n",
    "    hilo_final_1 = hilo_final_1[hilo_final_1['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    hilo_final_1.drop_duplicates(['DEALER_NAME', 'DEALER_ADDRESS_FULL']).shape\n",
    "\n",
    "    difference_df = pops_df_10mil.merge(hilo_final[['DEALER_NAME', 'DEALER_ADDRESS', 'DEALER_ADDRESS_FULL', 'MAKE',\n",
    "    'Local Radius', 'Local Population', 'N Local Dealers']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner').drop_duplicates()\n",
    "\n",
    "    difference_df.loc[:, ['Local Radius', 'N Local Dealers', 'Local Population']] = difference_df[['DISTANCE', 'DEALER_COUNT', 'people']].values\n",
    "    difference_df.drop(columns=['DISTANCE', 'DEALER_COUNT', 'people'], inplace=True)\n",
    "\n",
    "    hilo_final = pd.concat([hilo_final_1, difference_df], axis=0)\n",
    "    hilo_final.drop_duplicates(['DEALER_NAME', 'DEALER_ADDRESS_FULL']).shape\n",
    "\n",
    "    return hilo_final\n",
    "\n",
    "def low_values_fix(h4, d2d, hilo_final):\n",
    "\n",
    "    # print(h4.drop_duplicates(['DEALER_NAME', 'DEALER_ADDRESS_FULL']).shape)\n",
    "\n",
    "    d2d_low = d2d.merge(h4[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner')\n",
    "    d2d_low = d2d_low.drop_duplicates()\n",
    "    # Sort values by DEALER_ADDRESS_FULL and DISTANCE\n",
    "    df_sorted = d2d_low.sort_values(by=[\"DEALER_ADDRESS_FULL\", \"DISTANCE\"])\n",
    "    # Get the top 10 rows per group where DISTANCE ≤ 100 miles\n",
    "    df_top10 = df_sorted[df_sorted[\"DISTANCE\"] <= 100*1.60934].groupby([\"DEALER_NAME\",\"DEALER_ADDRESS_FULL\"]).head(10)\n",
    "    # Compute the maximum distance within these top 10 rows\n",
    "    df_low = df_top10.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"])[\"DISTANCE\"].max().reset_index()\n",
    "\n",
    "\n",
    "    # Fill NaN DISTANCE with 100 miles converted to km\n",
    "    df_low[\"DISTANCE\"] = df_low[\"DISTANCE\"].fillna(100 * 1.60934)\n",
    "    # Drop duplicate dealer comparisons once, before filtering\n",
    "    df_unique = df_sorted.drop_duplicates(subset=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"COMPARED_DEALER_ADDRESS_FULL\", \"COMPARED_DEALER_NAME\"])\n",
    "    # Filter df_sorted in advance for valid distances\n",
    "    # df_filtered = df_unique[df_unique[\"DISTANCE\"] <= df_low[\"DISTANCE\"].max()]\n",
    "    df_merged = df_unique.merge(df_low[['DEALER_NAME', 'DEALER_ADDRESS_FULL', 'DISTANCE']], \n",
    "                                on=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"], how=\"left\")\n",
    "    # Filter based on the dealer-specific DISTANCE condition\n",
    "    df_filtered = df_merged[df_merged[\"DISTANCE_x\"] <= df_merged[\"DISTANCE_y\"]]\n",
    "    # Count unique COMPARED_DEALERS per (DEALER_NAME, DEALER_ADDRESS_FULL)\n",
    "    df_counts = df_filtered.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"]).size().reset_index(name=\"DEALER_COUNT\")\n",
    "\n",
    "    # Merge precomputed counts with df_low\n",
    "    df_low = df_low.merge(df_counts, on=[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"], how=\"left\").fillna({\"DEALER_COUNT\": 0})\n",
    "    # Round up DISTANCE to the nearest integer\n",
    "    df_low[\"DISTANCE\"] = np.ceil(df_low[\"DISTANCE\"]).astype(int)\n",
    "    # Keep only relevant columns\n",
    "    df_low = df_low[[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"DISTANCE\", \"DEALER_COUNT\"]]\n",
    "    # assigning max distance = 100 miles and count of dealers = 0, to the dealers left out due to <100 miles condition\n",
    "\n",
    "    df_sorted_1 = df_sorted[['DEALER_NAME', 'DEALER_ADDRESS_FULL']].drop_duplicates()\n",
    "    df_top10_1 = df_top10[['DEALER_NAME', 'DEALER_ADDRESS_FULL']].drop_duplicates()\n",
    "\n",
    "    df_low1 = df_sorted_1.merge(df_top10_1, on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='left', indicator=True)\n",
    "    df_low1 = df_low1[df_low1['_merge']=='left_only'].drop(columns=['_merge'])\n",
    "    df_low1['DISTANCE'] = 100*1.60934\n",
    "    df_low1['DISTANCE'] = np.ceil(df_low1['DISTANCE']).astype(int)\n",
    "    df_low1['DEALER_COUNT'] = 0\n",
    "\n",
    "    # concating all\n",
    "    df_low = pd.concat([df_low, df_low1], axis=0)\n",
    "\n",
    "    df_low = df_low.merge(pops_df[['distance', 'people', 'DEALER_NAME', 'DEALER_ADDRESS_FULL']], left_on=['DEALER_NAME', 'DEALER_ADDRESS_FULL', 'DISTANCE'], right_on=['DEALER_NAME', 'DEALER_ADDRESS_FULL', 'distance'], how='left').sort_values(by='distance').drop(columns='distance')\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    hilo_final = hilo_final\n",
    "\n",
    "    hilo_final_1 = hilo_final.merge(df_low[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='left', indicator=True)\n",
    "\n",
    "    hilo_final_1 = hilo_final_1[hilo_final_1['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    hilo_final_1.DEALER_ADDRESS_FULL.nunique()\n",
    "\n",
    "    difference_df = df_low.merge(hilo_final[['DEALER_NAME', 'DEALER_ADDRESS', 'DEALER_ADDRESS_FULL', 'MAKE',\n",
    "    'Local Radius', 'Local Population', 'N Local Dealers']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner').drop_duplicates()\n",
    "\n",
    "    difference_df.loc[:, ['Local Radius', 'N Local Dealers', 'Local Population']] = difference_df[['DISTANCE', 'DEALER_COUNT', 'people']].values\n",
    "    difference_df.drop(columns=['DISTANCE', 'DEALER_COUNT', 'people'], inplace=True)\n",
    "\n",
    "    hilo_final = pd.concat([hilo_final_1, difference_df], axis=0)\n",
    "    hilo_final.DEALER_ADDRESS_FULL.nunique()\n",
    "    hilo_final.describe()\n",
    "\n",
    "    return hilo_final\n",
    "\n",
    "def population_constraint_fix_2(population_constraint, hilo_final, d2d, pops_df):\n",
    "\n",
    "    hilo_final_1_10mil = hilo_final[hilo_final['Local Population']>population_constraint]\n",
    "    hilo_final_1_10mil = hilo_final_1_10mil[['DEALER_NAME', 'DEALER_ADDRESS', 'DEALER_ADDRESS_FULL', 'Local Radius', 'N Local Dealers', 'Local Population']]\n",
    "    # print(hilo_final_1_10mil.info())\n",
    "\n",
    "    pops_df_10mil = pops_df.merge(hilo_final_1_10mil[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner')\n",
    "    pops_df_10mil = pops_df_10mil[(pops_df_10mil['distance']<100*1.60934) & (pops_df_10mil['people']<10000000)]\n",
    "    pops_df_10mil = (\n",
    "        pops_df_10mil.loc[pops_df_10mil.groupby([\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\"])[\"people\"].idxmax()]\n",
    "        [[\"DEALER_NAME\", \"DEALER_ADDRESS_FULL\", \"distance\", \"people\"]]\n",
    "    )\n",
    "\n",
    "    d2d_10mil = d2d.merge(pops_df_10mil[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner')\n",
    "    # Merge the two DataFrames on DEALER_ADDRESS_FULL\n",
    "    merged_df = pd.merge(d2d_10mil, pops_df_10mil, on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], suffixes=('_d2d', '_pops'))\n",
    "    # Filter rows where the DISTANCE in d2d_10mil is less than or equal to the distance in pops_df_10mil\n",
    "    filtered_df = merged_df[merged_df['DISTANCE'] <= merged_df['distance']]\n",
    "    # Count the number of COMPARED_DEALER_ADDRESS_FULL for each DEALER_ADDRESS_FULL\n",
    "    result = (\n",
    "        filtered_df.groupby(['DEALER_NAME', 'DEALER_ADDRESS_FULL'])\n",
    "        .agg({'COMPARED_DEALER_ADDRESS_FULL': 'count'})\n",
    "        .rename(columns={'COMPARED_DEALER_ADDRESS_FULL': 'count_compared_dealers'})\n",
    "    )\n",
    "    result = result.reset_index()\n",
    "\n",
    "    result2 = hilo_final_1_10mil.merge(result, on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='left', indicator=True)\n",
    "    result2 = result2[result2['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    result2 = result2[['DEALER_NAME', 'DEALER_ADDRESS_FULL']].drop_duplicates()\n",
    "    result2['count_compared_dealers'] = 0\n",
    "    result = pd.concat([result, result2], axis=0)\n",
    "    result = result.sort_values(by='count_compared_dealers', ascending=False)\n",
    "    result = result.drop_duplicates(subset=['DEALER_NAME', 'DEALER_ADDRESS_FULL'])\n",
    "    result.info()\n",
    "\n",
    "    pops_df_10mil = pops_df_10mil.merge(result, on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'])\n",
    "    pops_df_10mil.rename(columns={'distance':'DISTANCE', 'count_compared_dealers':'DEALER_COUNT'}, inplace=True)\n",
    "\n",
    "    hilo_final_1 = hilo_final.merge(pops_df_10mil[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='left', indicator=True)\n",
    "    hilo_final_1 = hilo_final_1[hilo_final_1['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    hilo_final_1.drop_duplicates(['DEALER_NAME', 'DEALER_ADDRESS_FULL']).shape\n",
    "\n",
    "    difference_df = pops_df_10mil.merge(hilo_final[['DEALER_NAME', 'DEALER_ADDRESS', 'DEALER_ADDRESS_FULL', 'MAKE',\n",
    "    'Local Radius', 'Local Population', 'N Local Dealers']], on=['DEALER_NAME', 'DEALER_ADDRESS_FULL'], how='inner').drop_duplicates()\n",
    "\n",
    "    difference_df.loc[:, ['Local Radius', 'N Local Dealers', 'Local Population']] = difference_df[['DISTANCE', 'DEALER_COUNT', 'people']].values\n",
    "    difference_df.drop(columns=['DISTANCE', 'DEALER_COUNT', 'people'], inplace=True)\n",
    "\n",
    "    hilo_final = pd.concat([hilo_final_1, difference_df], axis=0)\n",
    "    hilo_final.drop_duplicates(['DEALER_NAME', 'DEALER_ADDRESS_FULL']).shape\n",
    "\n",
    "    return hilo_final\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# Dealer Data Generation [Location Factors]\n",
    "################################################################################################\n",
    "def dealer_location_factors(all_dealers, dealers, coordinates, density, pops_df):\n",
    "    testing = False # True | False\n",
    "    test_address = '1001 S WHITE  ST, ATHENS, MC MINN, TN'\n",
    "    test_make = 'RAM'\n",
    "    \n",
    "    df = spark.createDataFrame(all_dealers).select('DEALER_ADDRESS_FULL', 'MAKE').join(\n",
    "        coordinates.dropna(), on=['DEALER_ADDRESS_FULL']\n",
    "    )\n",
    "    haversine_udf = F.udf(haversine_distance)\n",
    "\n",
    "    print('Applying Cross Join')\n",
    "    distance_df = df.alias('df1').crossJoin(df.alias('df2'))\n",
    "    distance_df = distance_df.where(\"df1.DEALER_ADDRESS_FULL != df2.DEALER_ADDRESS_FULL\")\n",
    "    \n",
    "    print('Calculating Distance')\n",
    "    distance_df = distance_df.withColumn('DISTANCE', haversine_udf('df1.LATITUDE', 'df1.LONGITUDE', 'df2.LATITUDE', 'df2.LONGITUDE'))\n",
    "    distance_df = distance_df.select(\n",
    "        F.col('df1.DEALER_ADDRESS_FULL').alias('DEALER_ADDRESS_FULL'), \n",
    "        F.col('df1.MAKE').alias('MAKE'), \n",
    "        F.col('df2.DEALER_ADDRESS_FULL').alias('COMPARED_DEALER_ADDRESS_FULL'), \n",
    "        F.col('df2.MAKE').alias('COMPARED_MAKE'), 'DISTANCE'\n",
    "    ).withColumn('DISTANCE', F.col('DISTANCE').cast('double'))\n",
    "    df = distance_df.join(\n",
    "        spark.createDataFrame(dealers).select('DEALER_ADDRESS_FULL', 'MAKE'), \n",
    "        on=['DEALER_ADDRESS_FULL', 'MAKE']\n",
    "    )\n",
    "    if testing:\n",
    "        df = df.filter(F.col('DEALER_ADDRESS_FULL') == test_address)\n",
    "        print(f'Dealer Data')\n",
    "        display(df)\n",
    "        print(f'Dealer Data - {test_make} Brand')\n",
    "        display(df.filter(F.col('COMPARED_MAKE') == test_make))\n",
    "\n",
    "    print('Calculating Distance Factor from population')\n",
    "    pops_df = pops_df.join(\n",
    "        spark.createDataFrame(dealers).select('DEALER_ADDRESS_FULL', 'MAKE', 'DEALER_STATE_ABBRV'), \n",
    "        on=['DEALER_ADDRESS_FULL']\n",
    "    ).withColumnRenamed('DEALER_STATE_ABBRV', 'state_code')\n",
    "    density = density.withColumn('Factor', F.col('pop2022') * 15 / F.col('n_dealers'))\n",
    "    pops_df = pops_df.withColumn('distance_mi', F.round(F.col('distance') * 0.62137119, 0))\n",
    "\n",
    "    pops_df = pops_df.select('MAKE', 'state_code', 'DEALER_ADDRESS_FULL', 'distance_mi', 'people').join(density.select('MAKE', 'state_code', 'Factor'), on=['MAKE', 'state_code'], how='left')\n",
    "    \n",
    "    local_pops_df = pops_df.filter(F.col('people') >= F.col('Factor')).orderBy(F.asc('distance_mi'))\n",
    "    local_pops_df = local_pops_df.dropDuplicates(subset=['DEALER_ADDRESS_FULL']).select('DEALER_ADDRESS_FULL', 'MAKE', 'distance_mi', 'people')\n",
    "    print('Done')\n",
    "    if testing:\n",
    "        print('Dealer Population Data - Local')\n",
    "        display(local_pops_df.filter(F.col('DEALER_ADDRESS_FULL') == test_address))\n",
    "        print('Dealer Population Data')\n",
    "        display(pops_df.filter(F.col('DEALER_ADDRESS_FULL') == test_address))\n",
    "\n",
    "    local_df = df.join(local_pops_df, on=['DEALER_ADDRESS_FULL', 'MAKE'])\n",
    "    if testing:\n",
    "        print(f'Dealer Data <-> Dealer Population Data - Local - {test_make} Brand')\n",
    "        display(local_df.filter((F.col('COMPARED_MAKE') == test_make)).orderBy(F.asc('DISTANCE')))\n",
    "        print(f'Dealer Data - {test_make} Brand')\n",
    "        display(df.filter((F.col('COMPARED_MAKE') == test_make)))\n",
    "        display(df.select('DEALER_ADDRESS_FULL').distinct())\n",
    "\n",
    "    cols = []\n",
    "    radius_range = [1, 2, 3, 4, 5, 7, 10, 15, 20, 30, 40, 50, 75, 100]\n",
    "    \n",
    "    # All Brands Calculations\n",
    "    print('Calculating for All Brands')\n",
    "    data = df\n",
    "    out = df.select('DEALER_ADDRESS_FULL').distinct()\n",
    "    print('out.count()', out.count())\n",
    "    for radius in radius_range:\n",
    "        col = f'ALL_BRAND_DEALERS_{radius}'\n",
    "        out = out.join(\n",
    "            data.filter(F.col('DISTANCE') <= radius).groupBy('DEALER_ADDRESS_FULL').agg(F.collect_list('COMPARED_DEALER_ADDRESS_FULL').alias(col)), \n",
    "            on=['DEALER_ADDRESS_FULL'], how='left'\n",
    "        )\n",
    "        cols.append(col)\n",
    "\n",
    "    data = local_df\n",
    "    out = out.join(\n",
    "        data.filter(F.col('DISTANCE') <= F.col('distance_mi')).groupBy('DEALER_ADDRESS_FULL').agg(\n",
    "            F.collect_list('COMPARED_DEALER_ADDRESS_FULL').alias('ALL_BRAND_DEALERS')\n",
    "        ), on=['DEALER_ADDRESS_FULL'], how='left'\n",
    "    )\n",
    "    cols.append('ALL_BRAND_DEALERS')\n",
    "\n",
    "    # Similar Brands Calculations\n",
    "    print('Calculating for Similar Brands')\n",
    "    sections = {\n",
    "        'Luxury': ['PORSCHE', 'MERCEDES-BENZ', 'LEXUS', 'BMW', 'AUDI', 'JAGUAR', 'LAND ROVER', 'VOLVO', 'ACURA', 'CADILLAC', 'LINCOLN', 'INFINITI', 'GENESIS', 'ALFA ROMEO', 'MASERATI'], \n",
    "        'Domestic': ['CHEVROLET', 'FORD', 'BUICK', 'GMC', 'CHRYSLER', 'DODGE', 'JEEP', 'RAM'],\n",
    "        'Import': ['TOYOTA', 'HONDA', 'SUBARU', 'HYUNDAI', 'KIA', 'VOLKSWAGEN', 'MAZDA', 'NISSAN', 'MITSUBISHI', 'MINI', 'FIAT' ,'SMARTCAR'],\n",
    "    }\n",
    "    data = local_df.filter(\n",
    "        (F.col('MAKE').isin(sections['Luxury']) & F.col('COMPARED_MAKE').isin(sections['Luxury'])) | \n",
    "        (F.col('MAKE').isin(sections['Domestic']) & F.col('COMPARED_MAKE').isin(sections['Domestic'])) | \n",
    "        (F.col('MAKE').isin(sections['Import']) & F.col('COMPARED_MAKE').isin(sections['Import']))\n",
    "    )\n",
    "\n",
    "    out = out.join(\n",
    "        data.filter(F.col('DISTANCE') <= F.col('distance_mi')).groupBy('DEALER_ADDRESS_FULL').agg(\n",
    "            F.collect_list('COMPARED_DEALER_ADDRESS_FULL').alias('SIMILAR_BRAND_DEALERS')\n",
    "        ), on=['DEALER_ADDRESS_FULL'], how='left'\n",
    "    )\n",
    "    cols.append('SIMILAR_BRAND_DEALERS')\n",
    "        \n",
    "    data = df.filter(\n",
    "        (F.col('MAKE').isin(sections['Luxury']) & F.col('COMPARED_MAKE').isin(sections['Luxury'])) | \n",
    "        (F.col('MAKE').isin(sections['Domestic']) & F.col('COMPARED_MAKE').isin(sections['Domestic'])) | \n",
    "        (F.col('MAKE').isin(sections['Import']) & F.col('COMPARED_MAKE').isin(sections['Import']))\n",
    "    )\n",
    "    for radius in radius_range:\n",
    "        col = f'SIMILAR_BRAND_DEALERS_{radius}'\n",
    "        out = out.join(data.filter(F.col('DISTANCE') <= radius).groupBy('DEALER_ADDRESS_FULL').agg(F.collect_list('COMPARED_DEALER_ADDRESS_FULL').alias(col)), on=['DEALER_ADDRESS_FULL'], how='left')\n",
    "        cols.append(col)\n",
    "\n",
    "    # Same Brand Calculations\n",
    "    print('Calculating for Same Brands')\n",
    "    data = local_df.filter(F.col('MAKE') == F.col('COMPARED_MAKE'))\n",
    "    \n",
    "    out = out.join(data.filter(F.col('DISTANCE') <= F.col('distance_mi')).groupBy('DEALER_ADDRESS_FULL').agg(F.collect_list('COMPARED_DEALER_ADDRESS_FULL').alias('BRAND_DEALERS')), on=['DEALER_ADDRESS_FULL'], how='left')\n",
    "    cols.append('BRAND_DEALERS')\n",
    "\n",
    "    data = df.filter(F.col('MAKE') == F.col('COMPARED_MAKE'))\n",
    "    if testing:\n",
    "        display(data.filter(F.col('DISTANCE')<=7))\n",
    "    for radius in radius_range:\n",
    "        col = f'BRAND_DEALERS_{radius}'\n",
    "        out = out.join(data.filter(F.col('DISTANCE') <= radius).groupBy('DEALER_ADDRESS_FULL').agg(F.collect_list('COMPARED_DEALER_ADDRESS_FULL').alias(col)), on=['DEALER_ADDRESS_FULL'], how='left')\n",
    "        cols.append(col)\n",
    "\n",
    "    out = out.join(\n",
    "        local_pops_df.select('DEALER_ADDRESS_FULL', 'distance_mi', 'people'), on=['DEALER_ADDRESS_FULL'], how='left'\n",
    "    ).withColumnRenamed('distance_mi', 'RADIUS').withColumnRenamed('people', 'POPULATION')\n",
    "    \n",
    "    stack_expr = \", \".join([f\"'{col}', {col}\" for col in cols])\n",
    "    out = out.selectExpr(\n",
    "        'DEALER_ADDRESS_FULL', 'RADIUS', 'POPULATION', \n",
    "        f\"stack({len(cols)}, {stack_expr}) as (Factor, List)\"\n",
    "    )\n",
    "    out = out.groupBy('DEALER_ADDRESS_FULL', 'RADIUS', 'POPULATION').agg(\n",
    "        F.map_from_entries(F.collect_list(F.struct(\"Factor\", \"List\"))).alias(\"Factors\")\n",
    "    ).select('DEALER_ADDRESS_FULL', 'RADIUS', 'POPULATION', 'Factors')\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "912ea983-65df-4bc2-9e0e-a99e97f73ac9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Coordinates Calculation"
    }
   },
   "outputs": [],
   "source": [
    "end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "start_date = end_date - relativedelta(years=2)\n",
    "start_date = datetime(start_date.year, start_date.month, 1)\n",
    "\n",
    "dealer_registry = pd.read_parquet(f'/dbfs/jump-datasets/pipeline_data/base_files/dealers_{start_date.strftime(\"%y%m\")}_{end_date.strftime(\"%y%m\")}.parquet')\n",
    "\n",
    "# dealer_coordinates = pd.read_parquet('/dbfs/jump-datasets/files/polk/all_new_dealers_coordinates.parquet')\n",
    "dealer_coordinates = pd.read_parquet('/dbfs/jump-datasets/pipeline_data/radius_calc/all_new_dealers_coordinates.parquet')\n",
    "dealer_coordinates = dealer_coordinates.drop_duplicates()\n",
    "\n",
    "out = generate_dealer_coordinates(dealer_registry, dealer_coordinates)\n",
    "out = out.drop_duplicates()\n",
    "\n",
    "savepath = f'/dbfs/jump-datasets/pipeline_data/radius_calc/all_new_dealers_coordinates.parquet'\n",
    "out.to_parquet(savepath)\n",
    "print(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c5ecaa1d-0d64-4b5b-889d-d73434dacdcf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate dealer population"
    }
   },
   "outputs": [],
   "source": [
    "end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "start_date = end_date - relativedelta(years=2)\n",
    "start_date = datetime(start_date.year, start_date.month, 1)\n",
    "\n",
    "dealer_registry = pd.read_parquet(f'/dbfs/jump-datasets/pipeline_data/base_files/dealers_{start_date.strftime(\"%y%m\")}_{end_date.strftime(\"%y%m\")}.parquet')\n",
    "dealer_coordinates = pd.read_parquet('/dbfs/jump-datasets/pipeline_data/radius_calc/all_new_dealers_coordinates.parquet')\n",
    "\n",
    "# pops_df = pd.read_parquet('/dbfs/jump-datasets/files/other/tomforth_data_2311.parquet')\n",
    "pops_df = pd.read_parquet(f'/dbfs/jump-datasets/pipeline_data/radius_calc/tomforth_data_{(end_date - relativedelta(months=1)).strftime(\"%y%m\")}.parquet')\n",
    "\n",
    "pops_df = generate_dealer_population(dealer_registry, dealer_coordinates, pops_df)\n",
    "\n",
    "savepath = f'/dbfs/jump-datasets/pipeline_data/radius_calc/tomforth_data_{end_date.strftime(\"%y%m\")}.parquet'\n",
    "pops_df.to_parquet(savepath, index=False)\n",
    "print(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "430ad900-7bd6-4be7-b43d-b6137558001e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optimal Radius for new dealers"
    }
   },
   "outputs": [],
   "source": [
    "end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "start_date = end_date - relativedelta(years=2)\n",
    "start_date = datetime(start_date.year, start_date.month, 1)\n",
    "\n",
    "################################################################################################\n",
    "state_codes = pd.read_excel('/dbfs/jump-datasets/files/other/US state codes.xlsx')\n",
    "dealer_coordinates = spark.read.parquet('dbfs:/jump-datasets/pipeline_data/radius_calc/all_new_dealers_coordinates.parquet')\n",
    "pop2022 = pd.read_excel(\"/dbfs/jump-datasets/files/other/POP2011-2023.xlsx\")[['State', 'pop2022']]\n",
    "pops_df = pd.read_parquet(f'/dbfs/jump-datasets/pipeline_data/radius_calc/tomforth_data_{end_date.strftime(\"%y%m\")}.parquet')\n",
    "selected_dealers = pd.read_parquet(f'/dbfs/jump-datasets/pipeline_data/base_files/dealers_{start_date.strftime(\"%y%m\")}_{end_date.strftime(\"%y%m\")}.parquet')\n",
    "selected_dealers = selected_dealers[selected_dealers['DEALER_STATE_ABBRV']!='PR']\n",
    "\n",
    "polk_df = spark.read.parquet(f'dbfs:/jump-datasets/datasets/polk_2019-01-01_{end}.parquet')\n",
    "polk_df = polk_df.filter(F.col('REPORT_YEAR_MONTH')<=end)\n",
    "\n",
    "################################################################################################\n",
    "## Count of nearby dealers for same state and make\n",
    "################################################################################################\n",
    "polk_df_n = polk_df.filter(F.col('REG_TYPE')=='N')\n",
    "count_df_pd = count_addresses_in_state_make(polk_df_n, selected_dealers)\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "optimal_radius_df_final = calculate_optimal_radius_new(selected_dealers, state_codes, count_df_pd, pop2022, polk_df, dealer_coordinates, pops_df)\n",
    "\n",
    "optimal_radius_df_final1 = optimal_radius_df_final.copy()\n",
    "print(optimal_radius_df_final1.info())\n",
    "\n",
    "end_time = time.time()\n",
    "time_spent = end_time - start_time\n",
    "\n",
    "print(time_spent)\n",
    "\n",
    "savepath = f'/dbfs/jump-datasets/pipeline_data/radius_calc/optimal_radius_df_{end_date.strftime(\"%y%m\")}.parquet'\n",
    "\n",
    "optimal_radius_df_final1.to_parquet(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "93cecf6c-28b6-461b-a41c-145e17838f4d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dealer to Dealer Distance"
    }
   },
   "outputs": [],
   "source": [
    "all_dealers = pd.read_parquet(\n",
    "    f'/dbfs/jump-datasets/pipeline_data/base_files/dealers_{start_date.strftime(\"%y%m\")}_{end_date.strftime(\"%y%m\")}.parquet'\n",
    ").drop(columns='SALES')\n",
    "\n",
    "coordinates = spark.read.parquet('dbfs:/jump-datasets/pipeline_data/radius_calc/all_new_dealers_coordinates.parquet')\n",
    "\n",
    "out = dealer_to_dealer_distance(all_dealers, coordinates)\n",
    "out = out.toPandas().drop_duplicates()\n",
    "\n",
    "#save to Parquet:\n",
    "out.to_parquet(f'/dbfs/FileStore/Manish Data/rough/dealer_to_dealer_distance_{end_date.strftime(\"%y%m\")}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "15984c6c-274b-4d76-8413-e57c3035de2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Tuning The Optimal Radius,  Local Dealers and Population"
    }
   },
   "outputs": [],
   "source": [
    "dealer_registry = pd.read_parquet(f'/dbfs/jump-datasets/pipeline_data/base_files/dealers_{start_date.strftime(\"%y%m\")}_{end_date.strftime(\"%y%m\")}_make_sales.parquet')\n",
    "\n",
    "pops_df = pd.read_parquet(f'/dbfs/jump-datasets/pipeline_data/radius_calc/tomforth_data_{end_date.strftime(\"%y%m\")}.parquet')\n",
    "pops_df = pops_df.merge(dealer_registry[['DEALER_NAME', 'DEALER_ADDRESS_FULL']], on='DEALER_ADDRESS_FULL', how='inner')\n",
    "\n",
    "optimal_radius_df_final1 = pd.read_parquet(f'/dbfs/jump-datasets/pipeline_data/radius_calc/optimal_radius_df_{end_date.strftime(\"%y%m\")}.parquet')\n",
    "\n",
    "optimal_radius_df_final1 = optimal_radius_df_final1.merge(dealer_registry[['DEALER_NAME', 'DEALER_ADDRESS', 'DEALER_ADDRESS_FULL', 'MAKE']], on = 'DEALER_ADDRESS_FULL', how='inner')\n",
    "optimal_radius_df_final1.rename(columns={'RADIUS':'Local Radius', 'POPULATION':'Local Population'}, inplace=True)\n",
    "optimal_radius_df_final1 = optimal_radius_df_final1.drop_duplicates()\n",
    "optimal_radius_df_final1 = optimal_radius_df_final1[['DEALER_NAME', 'DEALER_ADDRESS' ,'DEALER_ADDRESS_FULL', 'MAKE', 'Local Radius', 'Local Population']]\n",
    "\n",
    "d2d = pd.read_parquet(f'/dbfs/FileStore/Manish Data/rough/dealer_to_dealer_distance_{end_date.strftime(\"%y%m\")}.parquet')\n",
    "d2d['DISTANCE'] = d2d['DISTANCE']*1.60934\n",
    "d2d = d2d.drop_duplicates()\n",
    "\n",
    "optimal_radius_df_final1 = merge_and_get_dealers_count(optimal_radius_df_final1, d2d)\n",
    "\n",
    "df_30th_0th = fixing_more_than_30_local_dealers(optimal_radius_df_final1, d2d)\n",
    "\n",
    "hilo_final = update_original_df(optimal_radius_df_final1, df_30th_0th)\n",
    "\n",
    "population_constraint = 10000000\n",
    "\n",
    "hilo_final = population_constraint_fix(population_constraint, hilo_final, d2d, pops_df)\n",
    "\n",
    "h4 = hilo_final[\n",
    "    ((hilo_final['Local Radius'] <= 20 * 1.60934) & \n",
    "     (hilo_final['Local Population'] < 3000000) & \n",
    "     (hilo_final['N Local Dealers'] <= 9)) \n",
    "    |\n",
    "    ((hilo_final['N Local Dealers'] < 9) & \n",
    "     (hilo_final['Local Population'] <= 7000000))]\n",
    "\n",
    "hilo_final = low_values_fix(h4, d2d, hilo_final)\n",
    "\n",
    "hilo_final = population_constraint_fix_2(population_constraint, hilo_final, d2d, pops_df)\n",
    "\n",
    "hilo_final['Local Radius'] = hilo_final['Local Radius']*0.621371\n",
    "hilo_final['Local Radius'] = hilo_final['Local Radius'].astype(int)\n",
    "\n",
    "print(hilo_final.describe())\n",
    "hilo_final[['Local Radius', 'N Local Dealers', 'Local Population']].hist(bins=30, figsize=(12, 6))\n",
    "plt.show()\n",
    "\n",
    "hilo_final = hilo_final[['DEALER_NAME', 'DEALER_ADDRESS_FULL', 'Local Radius', 'Local Population', 'N Local Dealers']].rename(columns={'Local Radius':'RADIUS', 'Local Population':'POPULATION'})\n",
    "\n",
    "savepath = f'/dbfs/jump-datasets/pipeline_data/radius_calc/optimal_radius_df_{end_date.strftime(\"%y%m\")}.parquet'\n",
    "hilo_final.to_parquet(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "26d147f4-d2a9-40ec-953c-3b2bea5ec8ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dealer Location Factors"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "calc_type = 'location factors'\n",
    "\n",
    "date = next(item.name for item in dbutils.fs.ls('dbfs:/jump-datasets/pipeline_data/radius_calc/') if 'dealer_location_factors_' in item.name).split('_')[3] \n",
    "date = datetime.strptime(date, '%y%m').replace(day=1).strftime('%Y-%m-%d')\n",
    "date = (pd.to_datetime(date) + pd.DateOffset(months=6)).strftime('%y%m')\n",
    "\n",
    "if date == end_date.strftime(\"%y%m\"):\n",
    "    print(f'Calculating {calc_type} ...')\n",
    "    all_dealers = pd.read_parquet(f'/dbfs/jump-datasets/pipeline_data/base_files/dealers_{start_date.strftime(\"%y%m\")}_{end_date.strftime(\"%y%m\")}.parquet').drop(columns='SALES')\n",
    "    mapping = pd.read_parquet('/dbfs/jump-datasets/files/other/model_mapping_ram_added.parquet')                  \n",
    "    all_dealers = all_dealers[all_dealers['MAKE'].isin(list(mapping['Brand'].unique()))]\n",
    "    print(f'Dealers: {len(all_dealers):,}')\n",
    "\n",
    "    if calc_type == 'location factors':\n",
    "        pops_df = spark.read.parquet(f'dbfs:/jump-datasets/pipeline_data/radius_calc/tomforth_data_{end_date.strftime(\"%y%m\")}.parquet')\n",
    "        density = spark.read.parquet(f'dbfs:/jump-datasets/pipeline_data/base_files/state_brand_dealer_density_{end_date.strftime(\"%y%m\")}.parquet').select('Brand', 'state_code', 'n_dealers', 'pop2022').withColumnRenamed('Brand', 'MAKE')\n",
    "        coordinates = spark.read.parquet('dbfs:/jump-datasets/pipeline_data/radius_calc/all_new_dealers_coordinates.parquet')\n",
    "        for state_id in range(0, 9):\n",
    "            savepath = f'dbfs:/jump-datasets/pipeline_data/radius_calc/dealer_location_factors_{end_date.strftime(\"%y%m\")}_{state_names[state_id]}.parquet'\n",
    "            dealers = all_dealers[all_dealers['DEALER_STATE_ABBRV'].isin(states[state_id])].reset_index(drop=True)\n",
    "            print(f'{len(dealers)} - {list(dealers[\"DEALER_STATE_ABBRV\"].sort_values().unique())}')\n",
    "            \n",
    "            out = dealer_location_factors(all_dealers, dealers, coordinates, density, pops_df)\n",
    "            out.write.mode('overwrite').parquet(savepath)\n",
    "            print(f'Saved: {savepath}')\n",
    "else:\n",
    "    print(f\"This will be calculated in {datetime.strptime(date, '%y%m').strftime('%Y-%m')}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a581d504-30f1-407f-bfa2-b4ecae55fd59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) 3.1) Radius Calculations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}